{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ Exercise 1: Calculating Required Sample Size\n",
        "You are planning an A/B test to evaluate the impact of a new email subject line on the open rate. Based on past data, you expect a small effect size of 0.3 (an increase from 20% to 23% in the open rate). You aim for an 80% chance (power = 0.8) of detecting this effect if it exists, with a 5% significance level (Î± = 0.05).\n",
        "\n",
        "Calculate the required sample size per group using Pythonâ€™s statsmodels library.\n",
        "What sample size is needed for each group to ensure your test is properly powered?"
      ],
      "metadata": {
        "id": "Du-m4QY7os4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmJNN0tgoqMs",
        "outputId": "ee9f7f6f-a593-404d-ea1c-ae96531cc1fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required sample size per group: 175\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.stats.power import TTestIndPower\n",
        "\n",
        "effect_size = 0.3\n",
        "alpha = 0.05\n",
        "power = 0.8\n",
        "\n",
        "analysis = TTestIndPower()\n",
        "\n",
        "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
        "print(f\"Required sample size per group: {int(sample_size)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ  Exercise 2: Understanding the Relationship Between Effect Size and Sample Size\n",
        "Using the same A/B test setup as in Exercise 1, you want to explore how changing the expected effect size impacts the required sample size.\n",
        "\n",
        "Calculate the required sample size for the following effect sizes: 0.2, 0.4, and 0.5, keeping the significance level and power the same.\n",
        "How does the sample size change as the effect size increases? Explain why this happens.\n"
      ],
      "metadata": {
        "id": "fvNLhXogwVL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effect_sizes = [0.2, 0.4, 0.5]\n",
        "for effect_size in effect_sizes:\n",
        "    sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
        "    print(f\"Effect size: {effect_size}, Required sample size per group: {int(sample_size)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS_3KYS3xZog",
        "outputId": "1c1d57b6-6fef-4b6f-af42-53f96390e937"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effect size: 0.2, Required sample size per group: 393\n",
            "Effect size: 0.4, Required sample size per group: 99\n",
            "Effect size: 0.5, Required sample size per group: 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With higher Es lower sample size is required as larger changes less likely to be attributed to flactuations"
      ],
      "metadata": {
        "id": "7ezvygXA1vJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ Exercise 3: Exploring the Impact of Statistical Power\n",
        "Imagine you are conducting an A/B test where you expect a small effect size of 0.2. You initially plan for a power of 0.8 but wonder how increasing or decreasing the desired power level impacts the required sample size.\n",
        "\n",
        "Calculate the required sample size for power levels of 0.7, 0.8, and 0.9, keeping the effect size at 0.2 and significance level at 0.05.\n",
        "Question: How does the required sample size change with different levels of statistical power? Why is this understanding important when designing A/B tests?"
      ],
      "metadata": {
        "id": "ILS2ctp22Cw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "powers = [0.7, 0.8, 0.9]\n",
        "effect_size = 0.2\n",
        "for power in powers:\n",
        "    sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
        "    print(f\"Power: {power}, Required sample size per group: {int(sample_size)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHr2wMMg2DAo",
        "outputId": "b7518d5a-59ad-479f-deb7-4fcd02f3f276"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Power: 0.7, Required sample size per group: 309\n",
            "Power: 0.8, Required sample size per group: 393\n",
            "Power: 0.9, Required sample size per group: 526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher power - higher sample size required.\n",
        "I guess that it helps to better distribute usually limited resources on a test(to get samples) via balancing sample size that you get aquire and minimum power that you'd find sufficient"
      ],
      "metadata": {
        "id": "WPfkd3LLqwNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ðŸŒŸ Exercise 4: Implementing Sequential Testing\n",
        "You are running an A/B test on two versions of a product page to increase the purchase rate. You plan to monitor the results weekly and stop the test early if one version shows a significant improvement.\n",
        "\n",
        "Define your stopping criteria.\n",
        "Decide how you would implement sequential testing in this scenario.\n",
        "At the end of week three, Version B has a p-value of 0.02. What would you do next?"
      ],
      "metadata": {
        "id": "61EJ3-KFr0tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopping criteria: p-value < 0.05.\n",
        "\n",
        "Monitor results weekly, use cumulative data to calculate p-values.\n",
        "\n",
        "Decision at week three:\n",
        "    If the p-value is 0.02, stop the test as it meets the stopping criteria."
      ],
      "metadata": {
        "id": "x3iDUIswsOd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ  Exercise 5: Applying Bayesian A/B Testing\n",
        "Youâ€™re testing a new feature in your app, and you want to use a Bayesian approach. Initially, you believe the new feature has a 50% chance of improving user engagement. After collecting data, your analysis suggests a 65% probability that the new feature is better.\n",
        "\n",
        "Describe how you would set up your prior belief.\n",
        "After collecting data, how does the updated belief (posterior distribution) influence your decision?\n",
        "What would you do if the posterior probability was only 55%?"
      ],
      "metadata": {
        "id": "pPW_G29euqey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prior belief: 50% chance the new feature will improve user engagement.\n",
        "Update belief: 65% that new feature is better, therefore we can switch to it if our initial treshold allows it\n",
        "If it was only 55%, we would need to collect more data"
      ],
      "metadata": {
        "id": "HCdsfUf1uyau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ Exercise 6: Implementing Adaptive Experimentation\n",
        "Youâ€™re running a test with three different website layouts to increase user engagement. Initially, each layout gets 33% of the traffic. After the first week, Layout C shows higher engagement.\n",
        "\n",
        "Explain how you would adjust the traffic allocation after the first week.\n",
        "Describe how you would continue to adapt the experiment in the following weeks.\n",
        "What challenges might you face with adaptive experimentation, and how would you address them?"
      ],
      "metadata": {
        "id": "-bHFj-WGy-X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Allocate more traffic to the better-performing layout(C)\n",
        "2. Optimising, as the test goes, in case other layouts show better/worse output, send/take traffic accordingly. If no changes were shown during next week then additionally increase traffic sent to layout C.\n",
        "3. Early allocation changes might skew results, using other tests like Bayesian priot to changing traffic allocation might help\n",
        "Different layout might confuse some users, should be considered in std\n",
        "Upon taking too much traffic from layout it can impact factual results of the test, therefore needs to be set a minimum threshold for layout's traffic.\n"
      ],
      "metadata": {
        "id": "gzoFobyMzARg"
      }
    }
  ]
}